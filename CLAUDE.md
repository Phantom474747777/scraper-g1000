# CritterCaptures Lead Generation - Project Memory

## ğŸ“‹ Project Overview

**Project Name:** CritterCaptures Smart Lead Generation System  
**Purpose:** Interactive CLI tool that scrapes local business leads for wildlife/pest control services  
**Technology Stack:** Crawl4AI + InquirerPy (interactive CLI) + SQLite + Geopy  
**Target Area:** 50-mile radius from any US city (default: Dover, FL)  
**Data Source:** YellowPages.com

## ğŸ¯ What Makes This System Smart

### Interactive CLI Features:
1. **City-based ZIP lookup**: Enter any city/state, get all ZIPs within 50 miles
2. **Color-coded ZIP selection**:
   - ğŸŸ¢ Green = Fully available (no categories scraped)
   - ğŸŸ¡ Yellow = Partially used (some categories remaining)
   - ğŸ”´ Red = Fully used (all categories already scraped)
3. **Color-coded category selection**:
   - ğŸŸ¢ Green = Not yet scraped for this ZIP
   - ğŸ”´ Red = Already scraped (warns if user tries to re-scrape)
4. **Smart tracking**: `data/used_zips.json` prevents duplicate work
5. **No AI scoring needed**: Raw data â†’ you clean with ChatGPT Plus

## ğŸ—ï¸ Project Structure

```
ai-web-scraper/
â”œâ”€â”€ START.bat              # ğŸ‘ˆ DOUBLE-CLICK THIS to launch CLI
â”œâ”€â”€ leadgen_cli.py         # ğŸŒŸ NEW: Main interactive CLI entry point
â”œâ”€â”€ get_critter_leads.py   # Old entry point (kept for reference)
â”œâ”€â”€ requirements.txt       # Python dependencies
â”œâ”€â”€ README.md             # User documentation
â”œâ”€â”€ CLAUDE.md             # This file - project memory
â”œâ”€â”€ .gitignore            # Git ignore rules
â”œâ”€â”€ .env                  # Environment variables (optional - no AI keys needed!)
â”‚
â”œâ”€â”€ models/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â””â”€â”€ business.py       # Pydantic model for business data structure
â”‚
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ scraper.py        # Core scraping functions (browser config, LLM strategy)
â”‚   â”œâ”€â”€ database.py       # SQLite database for permanent lead storage
â”‚   â”œâ”€â”€ utils.py          # Utility functions (CSV, deduplication)
â”‚   â”œâ”€â”€ zip_lookup.py     # ğŸ†• ZIP code radius search using geopy
â”‚   â””â”€â”€ tracking.py       # ğŸ†• Track used ZIP+category combinations
â”‚
â””â”€â”€ data/
    â”œâ”€â”€ leads_tracker.db   # SQLite database (NEVER DELETE)
    â”œâ”€â”€ used_zips.json     # ğŸ†• Tracks which ZIP+category combos are used
    â””â”€â”€ {zip}_{category}_leads_{timestamp}.csv  # Output files
```

## ğŸ”‘ Environment Variables (Optional)

Previously required, now optional:
```
GEMINI_API_KEY=not_needed_anymore
GROQ_API_KEY=not_needed_anymore
```

**Why optional?** The scraper extracts data using Crawl4AI's built-in LLM, but we removed AI-based lead scoring. You clean leads yourself in ChatGPT Plus.

## ğŸ“¦ Dependencies

```
Crawl4AI==0.4.247        # AI-powered web scraping framework
python-dotenv==1.0.1     # Environment variable management
pydantic==2.10.6         # Data validation and modeling
InquirerPy==0.3.4        # ğŸ†• Interactive CLI with color support
geopy==2.4.1             # ğŸ†• Geocoding and distance calculations
requests>=2.31.0         # ğŸ†• HTTP requests for ZIP APIs
```

Install with: `pip install -r requirements.txt`

## ğŸš€ How to Use

### Quick Start:
1. **Double-click `START.bat`** in File Explorer
2. Enter your target city (e.g., "Dover")
3. Enter state code (e.g., "FL")
4. Select a ZIP code from the color-coded list
5. Select a category from the color-coded list
6. Confirm and watch it scrape!

### What Happens:
```
1. Find ZIPs â†’ System finds all ZIPs within 50 miles of your city
2. Show ZIPs â†’ Color-coded dropdown (green=available, red=used)
3. Select ZIP â†’ Pick which ZIP to scrape
4. Show Categories â†’ Color-coded by availability for that ZIP
5. Select Category â†’ Pick which business type to scrape
6. Scrape â†’ Automatically scrapes YellowPages
7. Clean â†’ Removes duplicates using database
8. Save â†’ Writes to data/{zip}_{category}_leads_{timestamp}.csv
9. Track â†’ Marks ZIP+category as "used" in used_zips.json
10. Continue? â†’ Option to scrape another combo
```

## ğŸ¨ Customer-Oriented Categories

These are **potential customers** who might need wildlife removal (NOT competitors):

1. Real Estate Agents
2. Property Managers
3. Home Inspectors
4. Construction Companies
5. Roofing Contractors
6. HVAC Services
7. Plumbing Companies
8. Landscaping Companies
9. Cleaning Services

## ğŸ“Š Output Format

**Filename Pattern:** `data/{zip}_{category}_leads_{timestamp}.csv`

**Example:** `data/33527_real_estate_agents_leads_2025-10-15_143022.csv`

**CSV Columns:**
- `name` - Business name
- `address` - Full address
- `phone_number` - Contact phone
- `website` - Business website URL
- `email` - Email address (if available, else "N/A")
- `category` - Which category was scraped
- `zip_code` - ZIP code scraped

## ğŸ—„ï¸ Tracking Systems

### 1. Database (`data/leads_tracker.db`)
**Purpose:** Permanent storage of all unique leads ever scraped

**Prevents:** Saving the same business twice (across all sessions)

### 2. JSON Tracker (`data/used_zips.json`)
**Purpose:** Track which ZIP+category combinations have been scraped

**Structure:**
```json
{
  "33527": {
    "categories": ["Real Estate Agents", "Property Managers"],
    "history": [
      {
        "category": "Real Estate Agents",
        "scraped_at": "2025-10-15T14:30:22",
        "leads_found": 45,
        "output_file": "data/33527_real_estate_agents_leads_2025-10-15_143022.csv"
      }
    ]
  }
}
```

**Prevents:** Re-scraping the same ZIP+category combo

## ğŸ¨ Color Coding System

### ZIP Code Colors:
- ğŸŸ¢ **Green**: All 9 categories available
- ğŸŸ¡ **Yellow**: Some categories used, some available (e.g., "3 categories available")
- ğŸ”´ **Red**: All categories already scraped

### Category Colors (for selected ZIP):
- ğŸŸ¢ **Green**: Not yet scraped for this ZIP
- ğŸ”´ **Red**: Already scraped for this ZIP (warns before allowing re-scrape)

## ğŸ”§ Configuration

**Search radius:** 50 miles (configurable in `zip_lookup.py`)

**Pages per search:** 2 (configurable in `leadgen_cli.py`)

**Known Tampa Bay ZIPs:** Pre-loaded in `zip_lookup.py` for Florida searches

## ğŸ§¹ Data Flow Pipeline

```
1. USER INPUT
   â””â”€> City: "Dover", State: "FL"

2. ZIP LOOKUP (geopy + haversine)
   â””â”€> Finds ~30 ZIPs within 50 miles

3. ZIP SELECTION (InquirerPy)
   â”œâ”€> Load tracking from used_zips.json
   â”œâ”€> Color-code each ZIP (green/yellow/red)
   â””â”€> User selects one ZIP

4. CATEGORY SELECTION (InquirerPy)
   â”œâ”€> Load categories for selected ZIP
   â”œâ”€> Color-code each category (green/red)
   â””â”€> User selects one category

5. SCRAPING (Crawl4AI)
   â””â”€> Scrapes YellowPages for selected ZIP+category

6. CLEANING (database.py)
   â”œâ”€> Remove duplicates within session
   â””â”€> Remove duplicates from database

7. SAVING (CSV)
   â””â”€> Write to data/{zip}_{category}_leads_{timestamp}.csv

8. TRACKING (tracking.py)
   â”œâ”€> Mark ZIP+category as "used"
   â”œâ”€> Save to used_zips.json
   â””â”€> Update database

9. CONTINUE?
   â””â”€> Prompt user to scrape another combo or exit
```

## ğŸ¨ Code Quality Standards

1. **PEP8 Compliance** - Follow Python style guidelines
2. **Modular Design** - Separate concerns (scraping, database, tracking, UI)
3. **Type Hints** - Use Pydantic for data validation
4. **Error Handling** - Graceful failures with try/except
5. **User Feedback** - Clear messages with emojis
6. **Interactive UX** - Color-coded, arrow-key navigation

## ğŸ”’ Important Rules

### DO:
âœ… Keep `data/leads_tracker.db` - permanent lead storage  
âœ… Keep `data/used_zips.json` - prevents duplicate work  
âœ… Use the interactive CLI (`leadgen_cli.py`) for all scraping  
âœ… Follow color codes (green=go, red=stop)  
âœ… Clean leads yourself in ChatGPT Plus (faster than AI scoring)

### DON'T:
âŒ Delete the database file (`leads_tracker.db`)  
âŒ Delete `used_zips.json` (you'll lose tracking)  
âŒ Re-scrape red-marked ZIP+category combos (wastes time)  
âŒ Expect AI scoring (we removed it - you'll clean manually)  
âŒ Commit CSV files to git (.gitignore handles this)

## ğŸ“ Logging & Output

The CLI uses emoji-based logging:
- ğŸ” = Searching/looking up
- âœ“ = Success
- âš ï¸ = Warning
- âŒ = Error/failure
- ğŸ’¾ = Saving data
- ğŸ§¹ = Cleaning data
- ğŸ“Š = Statistics
- ğŸŸ¢ğŸŸ¡ğŸ”´ = Status indicators

## ğŸ”„ Session History

**2025-10-15 (Morning):**
- âœ… Completed cleanup phase
- âœ… Fixed Python path in START.bat
- âœ… Removed AI scoring (GROQ/Gemini)

**2025-10-15 (Evening - MAJOR UPGRADE):**
- âœ… Created interactive CLI with color-coded dropdowns
- âœ… Added ZIP code radius search (50 miles)
- âœ… Implemented smart tracking system (used_zips.json)
- âœ… Fixed data saving bug (verified CSV creation)
- âœ… Updated categories to customer-oriented businesses
- âœ… Integrated InquirerPy for beautiful terminal UI
- âœ… Added geopy for geocoding
- âœ… System now prevents duplicate work automatically

## ğŸ’¡ Future Enhancements

Potential improvements:
- [ ] Export to Excel format
- [ ] Email validation/verification
- [ ] Bulk city import (scrape multiple cities at once)
- [ ] Visual progress bar for long scrapes
- [ ] Web dashboard (optional)
- [ ] Integration with CRM systems

## ğŸ†˜ Troubleshooting

**Issue:** "No ZIP codes found"
- Solution: Check city spelling, try nearby larger city

**Issue:** "All categories already scraped"  
- Solution: Select a different ZIP (green or yellow)

**Issue:** Import errors
- Solution: Run `pip install -r requirements.txt`

**Issue:** No CSV files created
- Solution: Check `data/` folder exists, verify file permissions

**Issue:** Colors not showing
- Solution: Windows Terminal or PowerShell 7+ recommended

## ğŸ“ How It Works Internally

### ZIP Lookup:
1. User enters "Dover, FL"
2. Geopy geocodes â†’ lat/lng coordinates
3. Haversine formula calculates distance to known FL ZIPs
4. Returns all ZIPs within 50 miles, sorted by distance

### Color Coding:
1. Load `used_zips.json`
2. For each ZIP, check how many categories used
3. Green if 0, Yellow if 1-8, Red if all 9

### Scraping:
1. Build YellowPages URL with ZIP+category
2. Crawl4AI extracts business data (name, phone, address, etc.)
3. Store in memory, dedupe against database
4. Write to CSV with timestamp
5. Mark as "used" in tracking

---

**Last Updated:** 2025-10-15  
**Status:** âœ… Production Ready (Smart CLI Upgrade)  
**Version:** 2.0 (Interactive CLI with Smart Tracking)
